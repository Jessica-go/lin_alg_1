{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82ad60c3",
   "metadata": {},
   "source": [
    "# What is PCA?\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a linear algebra method used to reduce the dimensionality of data while keeping as much variance (information) as possible.\n",
    "\n",
    "It works by:\n",
    "1. Centering the data (subtracting the mean)\n",
    "2. Computing the covariance matrix\n",
    "3. Finding the eigenvectors and eigenvalues\n",
    "4. Sorting the eigenvectors by highest variance (eigenvalues)\n",
    "5. Projecting the data onto the top principal components\n",
    "\n",
    "PCA transforms the data into a new coordinate system where the axes (called principal components) are ordered by importance. It is widely used in machine learning, data visualization, and noise reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1084879",
   "metadata": {},
   "source": [
    "# PCA via Linear Algebra "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb084c",
   "metadata": {},
   "source": [
    "\n",
    "Before using libraries like `scikit-learn`, it's important to understand how **Principal Component Analysis (PCA)** works under the hood. In this example, we manually implement PCA using only NumPy and linear algebra concepts, such as:\n",
    "\n",
    "- Centering the data\n",
    "- Covariance matrix\n",
    "- Eigenvalues and eigenvectors\n",
    "- Projection using dot products\n",
    "\n",
    "This helps us connect the math to the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d45e5",
   "metadata": {},
   "source": [
    "### Step 1â€“2: Generate and Center the Data\n",
    "\n",
    "We first create a 2D dataset with a clear linear pattern.  \n",
    "Then we **center** the data by subtracting the mean from each column.  \n",
    "This step is necessary because PCA is based on the **variance relative to the origin**.\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "X_{centered} = X - \\mu\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a4e4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f3eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Generate synthetic 2D data\n",
    "np.random.seed(0)\n",
    "x = 2 * np.random.rand(100, 1)\n",
    "y = 0.5 * x + np.random.randn(100, 1) * 0.2\n",
    "data = np.hstack((x, y))\n",
    "\n",
    "# Step 2: Center the data\n",
    "mean = np.mean(data, axis=0)\n",
    "data_centered = data - mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9202ef",
   "metadata": {},
   "source": [
    "### Step 3: Compute the Covariance Matrix\n",
    "\n",
    "The covariance matrix captures how the two dimensions (features) vary together.\n",
    "\n",
    "In 2D:\n",
    "$$\n",
    "\\text{Cov}(X) = \\begin{bmatrix}\n",
    "\\text{Var}(x_1) & \\text{Cov}(x_1, x_2) \\\\\n",
    "\\text{Cov}(x_2, x_1) & \\text{Var}(x_2)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We compute it with `np.cov(data_centered.T)`, where each row is a sample and each column is a feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d30e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Covariance matrix\n",
    "cov_matrix = np.cov(data_centered.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3656e11",
   "metadata": {},
   "source": [
    "### Step 4: Eigenvalues and Eigenvectors\n",
    "\n",
    "We compute the **eigenvalues** and **eigenvectors** of the covariance matrix. Each eigenvector represents a direction in the data space, and the corresponding eigenvalue tells us **how much variance** is in that direction.\n",
    "PCA keeps the directions (eigenvectors) with the largest eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Eigen decomposition\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Step 5: Sort eigenvalues and eigenvectors in descending order\n",
    "idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a55269",
   "metadata": {},
   "source": [
    "### Step 6: Projecting the data and ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034d15e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Projection onto the first principal component\n",
    "pc1 = eigenvectors[:, 0]\n",
    "projected = np.dot(data_centered, pc1.reshape(-1, 1))\n",
    "\n",
    "# Step 7: Recover projected data in original space (for visualization)\n",
    "projected_2D = np.dot(projected, pc1.reshape(1, -1)) + mean\n",
    "\n",
    "# Step 8: Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.4, label='Original data')\n",
    "plt.scatter(projected_2D[:, 0], projected_2D[:, 1], color='orange', alpha=0.8, label='PCA projection', marker='x')\n",
    "plt.quiver(*mean, *(pc1 * 3), color='purple', angles='xy', scale_units='xy', scale=1, width=0.01, label='First PC')\n",
    "plt.scatter(*mean, color='black', s=60, label='Data mean')\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Understanding PCA via Linear Algebra\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39800bb1",
   "metadata": {},
   "source": [
    "## Summary: What Did We Learn?\n",
    "\n",
    "By manually applying PCA:\n",
    "\n",
    "- We centered the data using the mean\n",
    "- We computed the covariance matrix\n",
    "- We extracted the principal directions using eigenvectors\n",
    "- We projected the data using dot products\n",
    "\n",
    "This gives a deeper understanding of what PCA really does: It finds a new axis (direction) that captures the most variance in the data.\n",
    "\n",
    "This understanding is crucial for interpreting PCA results in machine learning and data science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123a5e66",
   "metadata": {},
   "source": [
    "# PCA using scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4858ed",
   "metadata": {},
   "source": [
    "## PCA with scikit-learn\n",
    "\n",
    "In this section, we apply **Principal Component Analysis (PCA)** using the `scikit-learn` library. Unlike our previous implementation where we performed every step manually using NumPy and linear algebra, `scikit-learn` provides a compact and efficient way to do all steps in one call. This is useful when building machine learning pipelines or preprocessing real-world data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate synthetic 2D data\n",
    "np.random.seed(0)\n",
    "x = 2 * np.random.rand(100, 1)\n",
    "y = 0.5 * x + np.random.randn(100, 1) * 0.2\n",
    "data = np.hstack((x, y))\n",
    "\n",
    "# Step 2: Apply PCA using scikit-learn\n",
    "pca = PCA(n_components=1)\n",
    "projected = pca.fit_transform(data)  # this returns the 1D projection\n",
    "projected_back = pca.inverse_transform(projected)  # back to 2D for visualization\n",
    "\n",
    "# Step 3: Extract the principal component vector\n",
    "pc1 = pca.components_[0]\n",
    "mean = pca.mean_\n",
    "\n",
    "# Step 4: Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.4, label='Original data')\n",
    "plt.scatter(projected_back[:, 0], projected_back[:, 1], color='orange', alpha=0.8, label='PCA projection', marker='x')\n",
    "plt.quiver(*mean, *(pc1 * 3), color='purple', angles='xy', scale_units='xy', scale=1, width=0.01, label='PC1')\n",
    "plt.scatter(*mean, color='black', s=60, label='Data mean')\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"PCA using scikit-learn\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a283cc34",
   "metadata": {},
   "source": [
    "### How does `scikit-learn.PCA` work internally?\n",
    "\n",
    "The method `PCA.fit_transform(data)` performs:\n",
    "\n",
    "1. **Centering**: Subtracts the mean from each column of the data\n",
    "2. **Covariance calculation**: Computes the covariance matrix\n",
    "3. **Eigen decomposition**: Extracts eigenvalues and eigenvectors\n",
    "4. **Sorting**: Orders the components by the highest variance\n",
    "5. **Projection**: Projects the data onto the top `n_components`\n",
    "\n",
    "We can also **reconstruct** the original data (approximately) using `.inverse_transform()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c05d70",
   "metadata": {},
   "source": [
    "### What are we visualizing?\n",
    "\n",
    "In the plot:\n",
    "\n",
    "- The **blue points** are the original 2D data.\n",
    "- The **orange Xs** are the projections of the data onto the first principal component.\n",
    "- The **purple arrow** shows the direction of the first principal component (PC1).\n",
    "- The **black dot** is the mean of the data, used to center the points during PCA.\n",
    "\n",
    "This visualization shows how PCA finds the \"best-fit\" direction (in terms of variance) to reduce dimensionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30a077",
   "metadata": {},
   "source": [
    "### When to use `scikit-learn.PCA`?\n",
    "\n",
    "You should use `scikit-learn.PCA` in real-world projects when:\n",
    "\n",
    "- You want to **reduce the number of features** before applying machine learning\n",
    "- You want to **remove redundancy** or noise from your dataset\n",
    "- You want to **visualize high-dimensional data** in 2D or 3D\n",
    "- You want to **automate** PCA inside a preprocessing pipeline\n",
    "\n",
    "In educational settings, it's best to first **understand PCA with NumPy**, then move to `scikit-learn` to **apply it efficiently**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
